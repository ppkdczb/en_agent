"""
AI-generated question quality reviewer (考研英语).

This module reviews:
- Cloze tests (完形填空): `model.ClozeTest`
- Reading comprehension tasks (阅读理解): `model.ReadingTask`

It supports two layers:
1) Rule-based checks (deterministic): format, option sanity, basic heuristics.
2) Optional LLM-based judging (semantic): the LLM answers first, then gives
   quantitative scores and improvement suggestions.

The JSON report generated by `review_quality.py` stores:
- `rule_issues`: problems detected by rules (fatal/warning/info)
- `llm_judge`: structured judgement from the LLM (when enabled)
"""

import json
import re
from dataclasses import asdict, dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterable, List, Literal, Optional, Tuple, Union

from pydantic import BaseModel, Field, field_validator

from model import ClozeTest, ReadingTask
from utils import LLMService

Severity = Literal["fatal", "warning", "info"]


@dataclass(frozen=True)
class Issue:
    # A single rule-based issue.
    severity: Severity
    code: str
    message: str
    location: str = ""


def _add(issues: List[Issue], severity: Severity, code: str, message: str, location: str = "") -> None:
    issues.append(Issue(severity=severity, code=code, message=message, location=location))


def _tokenize_words(text: str) -> List[str]:
    # English word tokenizer used for rough length/quality heuristics.
    return re.findall(r"[A-Za-z]+(?:'[A-Za-z]+)?", text or "")


def _is_mostly_english(text: str, min_alpha_ratio: float = 0.6) -> bool:
    # Heuristic: measure ASCII a-z ratio (avoid counting Chinese as "alpha").
    if not text:
        return False
    ascii_letters = sum(("a" <= ch.lower() <= "z") for ch in text)
    spaces = sum(ch.isspace() for ch in text)
    denom = max(1, len(text) - spaces)
    return (ascii_letters / denom) >= min_alpha_ratio


def _detect_placeholders(text: str) -> List[int]:
    # Extract <question_i> indices from cloze content.
    return [int(m.group(1)) for m in re.finditer(r"<question_(\d+)>", text or "")]


def _has_duplicate_options(options: Dict[str, str]) -> bool:
    values = [v.strip().lower() for v in options.values()]
    values = [v for v in values if v]
    return len(values) != len(set(values))


def _choice_text(options: Dict[str, str], choice: str) -> Optional[str]:
    return options.get(choice) if options else None


def rule_check_cloze(item: ClozeTest) -> Tuple[List[Issue], Dict[str, Any]]:
    """
    Deterministic checks for cloze items:
    - placeholder/options/answers count match
    - option completeness and duplication
    - answer validity
    - basic language/length heuristics
    """
    issues: List[Issue] = []

    placeholders = _detect_placeholders(item.content)
    expected = len(item.answers)

    stats: Dict[str, Any] = {
        "placeholders_count": len(placeholders),
        "options_count": len(item.options),
        "answers_count": len(item.answers),
    }

    if not placeholders:
        _add(issues, "fatal", "CLOZE_NO_PLACEHOLDER", "No <question_i> placeholders found in content.")
        return issues, stats

    if len(placeholders) != len(item.options) or len(placeholders) != len(item.answers):
        _add(
            issues,
            "fatal",
            "CLOZE_COUNT_MISMATCH",
            f"Placeholder/options/answers count mismatch: placeholders={len(placeholders)}, options={len(item.options)}, answers={len(item.answers)}.",
        )

    # Placeholder indices should be 0..n-1 without gaps (recommended).
    unique_sorted = sorted(set(placeholders))
    if unique_sorted != list(range(0, len(unique_sorted))):
        _add(
            issues,
            "warning",
            "CLOZE_PLACEHOLDER_INDEX_NONCONSECUTIVE",
            f"Placeholder indices are not consecutive from 0: {unique_sorted}.",
        )

    # Options sanity.
    for i, opt in enumerate(item.options):
        opt_dict = opt.model_dump()
        if any(not str(opt_dict.get(k, "")).strip() for k in ("A", "B", "C", "D")):
            _add(issues, "fatal", "CLOZE_OPTION_EMPTY", "One or more options are empty.", location=f"blank[{i}].options")
        if _has_duplicate_options(opt_dict):
            _add(issues, "warning", "CLOZE_OPTION_DUPLICATE", "Duplicate option texts found (A/B/C/D).", location=f"blank[{i}].options")

        # Very short / non-word options are suspicious for exam-level cloze.
        for k, v in opt_dict.items():
            vv = (v or "").strip()
            if len(vv) <= 1:
                _add(issues, "warning", "CLOZE_OPTION_TOO_SHORT", f"Option {k} is too short: '{vv}'.", location=f"blank[{i}].options.{k}")
            if not re.search(r"[A-Za-z]", vv):
                _add(issues, "warning", "CLOZE_OPTION_NO_ALPHA", f"Option {k} contains no letters: '{vv}'.", location=f"blank[{i}].options.{k}")

    # Answer validity + "answer text duplication" heuristic.
    for i, ans in enumerate(item.answers):
        if ans not in {"A", "B", "C", "D"}:
            _add(issues, "fatal", "CLOZE_ANSWER_INVALID", f"Answer must be one of A/B/C/D, got '{ans}'.", location=f"blank[{i}].answer")
            continue
        opt_dict = item.options[i].model_dump() if i < len(item.options) else {}
        ans_text = _choice_text(opt_dict, ans)
        if ans_text is not None and ans_text.strip() == "":
            _add(issues, "fatal", "CLOZE_ANSWER_POINTS_TO_EMPTY", "Answer points to an empty option.", location=f"blank[{i}]")

    # Language heuristics for content.
    if not _is_mostly_english(item.content):
        _add(issues, "warning", "CLOZE_CONTENT_NOT_ENGLISH_LIKE", "Content does not look like English (low alphabet ratio).", location="content")
    if len(_tokenize_words(item.content)) < 120:
        _add(issues, "info", "CLOZE_CONTENT_SHORT", "Cloze passage seems short; consider exam-level length.", location="content")

    stats["fatal_count"] = sum(1 for x in issues if x.severity == "fatal")
    stats["warning_count"] = sum(1 for x in issues if x.severity == "warning")
    stats["info_count"] = sum(1 for x in issues if x.severity == "info")
    return issues, stats


def rule_check_reading(task: ReadingTask) -> Tuple[List[Issue], Dict[str, Any]]:
    """
    Deterministic checks for reading tasks:
    - passage length/format heuristics
    - question id uniqueness
    - option completeness and duplication
    - answer validity
    """
    issues: List[Issue] = []

    words = _tokenize_words(task.content)
    stats: Dict[str, Any] = {
        "word_count": len(words),
        "question_count": len(task.questions),
    }

    if not task.content.strip():
        _add(issues, "fatal", "READING_EMPTY_CONTENT", "Reading passage content is empty.", location="content")

    if len(words) < 180:
        _add(issues, "warning", "READING_PASSAGE_SHORT", "Passage may be short for 考研-style reading.", location="content")
    if len(words) > 650:
        _add(issues, "info", "READING_PASSAGE_LONG", "Passage is long; consider time constraints.", location="content")

    # Rough signal for spacing/OCR issues: too many long tokens.
    long_tokens = [w for w in words if len(w) >= 18]
    if len(long_tokens) >= max(6, len(words) * 0.02):
        _add(
            issues,
            "warning",
            "READING_MANY_LONG_TOKENS",
            "Many unusually long tokens detected; possible missing spaces/OCR issues.",
            location="content",
        )

    seen_ids: set[int] = set()
    for q in task.questions:
        if q.id in seen_ids:
            _add(issues, "fatal", "READING_DUPLICATE_QID", f"Duplicate question id: {q.id}.", location="questions")
        seen_ids.add(q.id)

        opt_dict = q.options.model_dump()
        if any(not str(opt_dict.get(k, "")).strip() for k in ("A", "B", "C", "D")):
            _add(issues, "fatal", "READING_OPTION_EMPTY", "One or more options are empty.", location=f"q[{q.id}].options")
        if _has_duplicate_options(opt_dict):
            _add(issues, "warning", "READING_OPTION_DUPLICATE", "Duplicate option texts found (A/B/C/D).", location=f"q[{q.id}].options")
        if q.answer not in {"A", "B", "C", "D"}:
            _add(issues, "fatal", "READING_ANSWER_INVALID", f"Answer must be one of A/B/C/D, got '{q.answer}'.", location=f"q[{q.id}].answer")

        if len(_tokenize_words(q.prompt)) < 5:
            _add(issues, "warning", "READING_PROMPT_TOO_SHORT", "Question prompt seems too short/underspecified.", location=f"q[{q.id}].prompt")

    stats["fatal_count"] = sum(1 for x in issues if x.severity == "fatal")
    stats["warning_count"] = sum(1 for x in issues if x.severity == "warning")
    stats["info_count"] = sum(1 for x in issues if x.severity == "info")
    return issues, stats


class ClozeLLMJudgement(BaseModel):
    """
    LLM judgement for cloze.

    Notes on fields:
    - `predicted_answers`/`confidence`: the reviewer LLM's own answers and certainty.
    - `overall_scores`: 0-10 dimension scores (you can use in论文/汇总统计).
    - `per_blank`: per-blank quantitative scores + short rationale (`notes`).
    """
    class BlankScore(BaseModel):
        index: int
        predicted: Literal["A", "B", "C", "D"]
        confidence: float = Field(description="Confidence in [0,1].")
        matches_provided: bool
        scores: Dict[str, int] = Field(
            description="0-10 integers with keys: option_quality, blank_quality, difficulty, clarity."
        )
        notes: str = Field(description="1-2 sentences justifying the scores.")

    predicted_answers: List[Literal["A", "B", "C", "D"]] = Field(description="Predicted answers aligned to blanks.")
    confidence: List[float] = Field(description="Per blank confidence in [0,1].")
    answer_mismatch_indices: List[int] = Field(description="0-based indices where predicted != provided answer.")
    overall_scores: Dict[str, int] = Field(
        description="0-10 integers with keys: option_setting, question_quality, difficulty, language, coherence."
    )
    overall_score: float = Field(description="Overall score in [0,100].")
    per_blank: List[BlankScore] = Field(description="Per-blank quantitative review.")
    fatal_issues: List[str]
    improvements: List[str]

    @field_validator("fatal_issues", "improvements", mode="before")
    @classmethod
    def coerce_list_fields(cls, v: Any):
        if v is None:
            return []
        if isinstance(v, list):
            return [str(x).strip() for x in v if str(x).strip()]
        if isinstance(v, str):
            text = v.strip()
            if not text:
                return []
            parts = re.split(r"(?:\r?\n)+|;+", text)
            cleaned: List[str] = []
            for p in parts:
                p = re.sub(r"^\s*\d+\s*[\.\)、\-:：]\s*", "", p.strip())
                if p:
                    cleaned.append(p)
            return cleaned
        return [str(v).strip()] if str(v).strip() else []


class ReadingLLMJudgement(BaseModel):
    """
    LLM judgement for reading comprehension.

    - `predicted_answers`: {question_id: choice}
    - `per_question`: per-question scores and short rationale (`notes`)
    """
    class QuestionScore(BaseModel):
        id: int
        predicted: Literal["A", "B", "C", "D"]
        confidence: float = Field(description="Confidence in [0,1].")
        matches_provided: bool
        scores: Dict[str, int] = Field(
            description="0-10 integers with keys: option_quality, question_quality, grounding, difficulty, clarity."
        )
        notes: str = Field(description="1-2 sentences justifying the scores.")

    predicted_answers: Dict[int, Literal["A", "B", "C", "D"]] = Field(description="Dict mapping question id -> A/B/C/D.")
    confidence: Dict[int, float] = Field(description="Per question confidence in [0,1].")
    answer_mismatch_ids: List[int] = Field(description="Question ids where predicted != provided answer.")
    overall_scores: Dict[str, int] = Field(
        description="0-10 integers with keys: option_setting, question_quality, grounding, difficulty, language."
    )
    overall_score: float = Field(description="Overall score in [0,100].")
    per_question: List[QuestionScore] = Field(description="Per-question quantitative review.")
    fatal_issues: List[str]
    improvements: List[str]

    @field_validator("fatal_issues", "improvements", mode="before")
    @classmethod
    def coerce_list_fields(cls, v: Any):
        if v is None:
            return []
        if isinstance(v, list):
            return [str(x).strip() for x in v if str(x).strip()]
        if isinstance(v, str):
            text = v.strip()
            if not text:
                return []
            parts = re.split(r"(?:\r?\n)+|;+", text)
            cleaned: List[str] = []
            for p in parts:
                p = re.sub(r"^\s*\d+\s*[\.\)、\-:：]\s*", "", p.strip()) # Remove leading numbering or bullet points
                if p:
                    cleaned.append(p)
            return cleaned
        return [str(v).strip()] if str(v).strip() else []


def _scores_0_10_to_0_100(scores: Dict[str, int], weights: Optional[Dict[str, float]] = None) -> float:
    # Utility to aggregate 0-10 dimension scores into a 0-100 score.
    if not scores:
        return 0.0
    weights = weights or {k: 1.0 for k in scores.keys()}
    total_w = sum(weights.get(k, 0.0) for k in scores.keys())
    if total_w <= 0:
        return 0.0
    acc = 0.0
    for k, v in scores.items():
        w = weights.get(k, 0.0)
        vv = float(max(0, min(10, int(v))))
        acc += w * vv
    return (acc / (10.0 * total_w)) * 100.0


def _normalize_cloze_judgement(item: ClozeTest, judge: ClozeLLMJudgement) -> ClozeLLMJudgement:
    """
    Normalize LLM output so indices always start from 0 per item.

    Some models output 1-based indices, global indices, or inconsistent per_blank.
    This function rebuilds:
    - `answer_mismatch_indices` (0-based)
    - `per_blank[i].index == i`
    - `per_blank[i].matches_provided` based on provided answers
    It also clamps lengths to the number of blanks.
    """
    n = len(item.answers)

    predicted = list(judge.predicted_answers)[:n]
    if len(predicted) < n:
        predicted += list(item.answers[len(predicted) :])

    confidence = list(judge.confidence)[:n]
    if len(confidence) < n:
        confidence += [0.0] * (n - len(confidence))
    confidence = [float(max(0.0, min(1.0, c))) for c in confidence]

    # Reuse LLM-provided per_blank info when possible, but enforce indices.
    per_blank_by_index: Dict[int, ClozeLLMJudgement.BlankScore] = {}
    for b in judge.per_blank or []:
        if 0 <= int(b.index) < n:
            per_blank_by_index[int(b.index)] = b

    rebuilt: List[ClozeLLMJudgement.BlankScore] = []
    mismatches: List[int] = []
    for i in range(n):
        pred = predicted[i]
        matches = (pred == item.answers[i])
        if not matches:
            mismatches.append(i)

        existing = per_blank_by_index.get(i)
        scores = existing.scores if existing else {}
        notes = existing.notes if existing else ""

        rebuilt.append(
            ClozeLLMJudgement.BlankScore(
                index=i,
                predicted=pred,
                confidence=confidence[i],
                matches_provided=matches,
                scores=scores,
                notes=notes,
            )
        )

    overall_score = float(judge.overall_score)
    if not (0.0 <= overall_score <= 100.0):
        overall_score = _scores_0_10_to_0_100(judge.overall_scores)

    return judge.model_copy(
        update={
            "predicted_answers": predicted,
            "confidence": confidence,
            "answer_mismatch_indices": mismatches,
            "overall_score": overall_score,
            "per_blank": rebuilt,
        }
    )


def _normalize_reading_judgement(task: ReadingTask, judge: ReadingLLMJudgement) -> ReadingLLMJudgement:
    """
    Normalize LLM output so per_question follows task order and is complete.

    - ensures `per_question` contains every question id exactly once
    - rebuilds `answer_mismatch_ids`
    - clamps/confidence to [0,1]
    """
    qids = [q.id for q in task.questions]
    provided = {q.id: q.answer for q in task.questions}

    predicted = dict(judge.predicted_answers or {})
    confidence = {int(k): float(v) for k, v in (judge.confidence or {}).items()}
    confidence = {k: float(max(0.0, min(1.0, v))) for k, v in confidence.items()}

    existing_by_id: Dict[int, ReadingLLMJudgement.QuestionScore] = {int(q.id): q for q in (judge.per_question or [])}

    rebuilt: List[ReadingLLMJudgement.QuestionScore] = []
    mismatches: List[int] = []
    for qid in qids:
        pred = predicted.get(qid, provided[qid])
        conf = confidence.get(qid, 0.0)
        matches = (pred == provided[qid])
        if not matches:
            mismatches.append(qid)

        existing = existing_by_id.get(qid)
        scores = existing.scores if existing else {}
        notes = existing.notes if existing else ""

        rebuilt.append(
            ReadingLLMJudgement.QuestionScore(
                id=qid,
                predicted=pred,
                confidence=conf,
                matches_provided=matches,
                scores=scores,
                notes=notes,
            )
        )

    overall_score = float(judge.overall_score)
    if not (0.0 <= overall_score <= 100.0):
        overall_score = _scores_0_10_to_0_100(judge.overall_scores)

    # Keep predicted_answers/confidence consistent with rebuilt.
    predicted_out = {x.id: x.predicted for x in rebuilt}
    confidence_out = {x.id: x.confidence for x in rebuilt}

    return judge.model_copy(
        update={
            "predicted_answers": predicted_out,
            "confidence": confidence_out,
            "answer_mismatch_ids": mismatches,
            "overall_score": overall_score,
            "per_question": rebuilt,
        }
    )


def llm_judge_cloze(item: ClozeTest, llm: LLMService) -> ClozeLLMJudgement:
    system = (
        "You are an expert English exam item reviewer for Chinese postgraduate entrance exams (考研英语). "
        "Judge whether a cloze passage and its options/answers are high-quality, unambiguous, and appropriately difficult."
    )
    user = {
        "task": "review_cloze_quality",
        "passage": item.content,
        "options": [o.model_dump() for o in item.options],
        "provided_answers": item.answers,
        "output_requirements": {
            "predicted_answers": "Choose A/B/C/D for EACH blank independently, based on best fit.",
            "confidence": "List floats in [0,1] for each blank.",
            "answer_mismatch_indices": "0-based indices where your predicted answer differs from provided_answers.",
            "overall_scores": "0-10 integers with keys: option_setting, question_quality, difficulty, language, coherence.",
            "overall_score": "A float in [0,100]. Prefer computing it consistently from overall_scores.",
            "per_blank": "List per blank with: index, predicted, confidence, matches_provided, scores(0-10 keys: option_quality, blank_quality, difficulty, clarity), notes.",
            "fatal_issues": "List of fatal problems (if any).",
            "improvements": "Actionable improvements.",
        },
        "constraints": [
            "Return ONLY the structured output; no extra keys.",
            "If multiple answers could fit, mark validity low and list it in fatal_issues.",
        ],
    }
    judge = llm.invoke(user_prompt=json.dumps(user, ensure_ascii=False), system_prompt=system, pydantic_obj=ClozeLLMJudgement)
    return _normalize_cloze_judgement(item, judge)


def llm_judge_reading(task: ReadingTask, llm: LLMService) -> ReadingLLMJudgement:
    system = (
        "You are an expert English exam item reviewer for Chinese postgraduate entrance exams (考研英语). "
        "Judge whether a reading passage and its multiple-choice questions are high-quality, grounded in the passage, "
        "unambiguous, and appropriately difficult."
    )
    user = {
        "task": "review_reading_quality",
        "title": task.title,
        "passage": task.content,
        "questions": [
            {"id": q.id, "prompt": q.prompt, "options": q.options.model_dump(), "provided_answer": q.answer}
            for q in task.questions
        ],
        "output_requirements": {
            "predicted_answers": "Dict mapping question id -> A/B/C/D.",
            "confidence": "Dict mapping question id -> float in [0,1].",
            "answer_mismatch_ids": "List question ids where your predicted answer differs from provided_answer.",
            "overall_scores": "0-10 integers with keys: option_setting, question_quality, grounding, difficulty, language.",
            "overall_score": "A float in [0,100]. Prefer computing it consistently from overall_scores.",
            "per_question": "List per question with: id, predicted, confidence, matches_provided, scores(0-10 keys: option_quality, question_quality, grounding, difficulty, clarity), notes.",
            "fatal_issues": "List of fatal problems (if any).",
            "improvements": "Actionable improvements.",
        },
        "constraints": [
            "Answer ONLY based on the passage; if not supported, mark grounding low and mention in fatal_issues.",
            "Return ONLY the structured output; no extra keys.",
        ],
    }
    judge = llm.invoke(user_prompt=json.dumps(user, ensure_ascii=False), system_prompt=system, pydantic_obj=ReadingLLMJudgement)
    return _normalize_reading_judgement(task, judge)


def overall_score(rule_issues: List[Issue], llm_overall_score: Optional[float] = None) -> float:
    if any(x.severity == "fatal" for x in rule_issues):
        return 0.0
    base = 85.0
    base -= 8.0 * sum(1 for x in rule_issues if x.severity == "warning")
    base -= 2.0 * sum(1 for x in rule_issues if x.severity == "info")
    base = max(0.0, min(100.0, base))

    if llm_overall_score is not None:
        llm_score = float(max(0.0, min(100.0, llm_overall_score)))
        return 0.4 * base + 0.6 * llm_score
    return base


def dump_report_json(path: Union[str, Path], report: Dict[str, Any]) -> None:
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(report, ensure_ascii=False, indent=2), encoding="utf-8")


def now_tag() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def issues_to_dict(issues: List[Issue]) -> List[Dict[str, Any]]:
    return [asdict(x) for x in issues]
